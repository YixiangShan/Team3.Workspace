{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YixiangShan/Team3.Workspace/blob/main/FlowerDataMLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading and importing the data\n",
        "\n",
        "The first step is to download and extract the data. We do this directly from the URL below.\n",
        "\n",
        "After you run the cell below you should see a folder called 'FlowerData' in the file browser to the left of the screen. Click the folder icon to see your files, and maybe the refresh icon to double check you have the data. The images themselves should be in a folder called 'jpg' within 'FlowerData'."
      ],
      "metadata": {
        "id": "ujQ3nC9-2DBY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLWeGs9n2Acu"
      },
      "outputs": [],
      "source": [
        "import os, requests, tarfile\n",
        "\n",
        "try:\n",
        "  os.mkdir('FlowerData')\n",
        "except:\n",
        "  pass\n",
        "\n",
        "url = 'https://www.robots.ox.ac.uk/~vgg/data/flowers/17/17flowers.tgz'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open('FlowerData/FlowerData.tgz', 'wb').write(r.content)\n",
        "\n",
        "file = tarfile.open('FlowerData/FlowerData.tgz')\n",
        "file.extractall('FlowerData')\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract data from images\n",
        "\n",
        "The next step is to extract the raw data we want from the image files. The full dataset has 17 species or categories overall, arranged in blocks of 80 examples, but we will just work on a selected subset of 4 species.\n",
        "\n",
        "The code cell below performs the following steps:\n",
        "\n",
        "1. Get a list of all the filenames in the 'jpg' folder, and pick a selected subset of them.\n",
        "\n",
        "2. Set a image size, and so calculate the number of features, and create a features matrix $X$ (initially full of zeros).\n",
        "\n",
        "3. Loop through all the selected images, resize, flatten and add the data to $X$.\n",
        "\n",
        "4. Make the target vector $y$ using the names of the selected species.\n",
        "\n",
        "5. Print out some information of the shape of $X$ and $y$."
      ],
      "metadata": {
        "id": "6wYVA5HR2KQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "img_list_full = sorted(glob.glob('FlowerData/jpg/*.jpg')) # need to sort to ensure in order\n",
        "\n",
        "full_species_list = ['Daffodil','Snowdrop','LilyValley','Bluebell','Crocus',\n",
        "                     'Iris','Tigerlily','Tulip','Fritillary','Sunflower',\n",
        "                     'Daisy','ColtsFoot','Dandelion','Cowslip','Buttercup',\n",
        "                     'Windflower','Pansy']\n",
        "\n",
        "n_species = 4\n",
        "\n",
        "img_list = img_list_full[:80]+img_list_full[480:640]+img_list_full[800:880] # just take the species in offline example\n",
        "#img_list = img_list_full[:n_species*80]\n",
        "\n",
        "nrow = 128\n",
        "ncol = 128\n",
        "\n",
        "n_features = nrow * ncol * 3\n",
        "n_samples = len(img_list)\n",
        "X = np.zeros((n_samples,n_features))\n",
        "\n",
        "\n",
        "for i,im_name in enumerate(img_list):\n",
        "\n",
        "    img = cv2.imread(im_name)\n",
        "\n",
        "    # resize/reshape\n",
        "    img_resized = cv2.resize(img,(nrow,ncol))\n",
        "\n",
        "    # flatten into array\n",
        "    img_flat = img_resized.flatten()\n",
        "\n",
        "    X[i,:] = img_flat\n",
        "\n",
        "\n",
        "species_list = ['Daffodil','Tigerlily','Tulip','Daisy']\n",
        "#species_list = full_species_list[:n_species]\n",
        "\n",
        "y=[]\n",
        "\n",
        "for species in species_list:\n",
        "    y=y+[species]*80\n",
        "\n",
        "\n",
        "print('features matrix has shape',X.shape)\n",
        "print('target vector has length',len(y))"
      ],
      "metadata": {
        "id": "SsfvQ_C72ND1",
        "outputId": "426f419d-67cc-4f16-bfa0-d8c5526a10bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features matrix has shape (320, 49152)\n",
            "target vector has length 320\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Multilayer perceptron\n",
        "\n",
        "Using Scikit-Learn, split the data into training and validation sets, and set up and train a multilayer perceptron with two hidden layers, of size 256 and 64.\n",
        "\n",
        "This might take a minute or so to run so please be patient! The progress should be displayed at the bottom of the screen."
      ],
      "metadata": {
        "id": "eZhzdkRP2WlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split into training / validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y,random_state=13)\n",
        "\n",
        "# choose / set up model\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "model = MLPClassifier(hidden_layer_sizes=(256,64,),max_iter=1000)\n",
        "\n",
        "model.fit(Xtrain, ytrain)"
      ],
      "metadata": {
        "id": "ijOgpApg2ZwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate and visualise the results\n",
        "\n",
        "Use the trained model to make a set of predictions on the test set, evaluate the accuracy score, and create and display the confusion matrix."
      ],
      "metadata": {
        "id": "41YgCkEU2inW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_model = model.predict(Xtest)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "score = accuracy_score(ytest, y_model)\n",
        "print(score)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "mat = confusion_matrix(ytest, y_model)\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=mat,display_labels=species_list)\n",
        "disp.plot()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1sS4zJwc2ls2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Things to try\n",
        "\n",
        "Hopefully this exercise gave you a good idea of how you might use colab, in particular for machine learning. If you want to, and have time, you can try saving your own version of code (click the \"Copy to Drive\" button near the top of this window), and try apapting the code to do any of the following:\n",
        "\n",
        "* Train and evaluate the model using a different random split in the data (change 'random_state' in the function call to 'train_test_split')\n",
        "\n",
        "* Change the number and size of hidden layers in the multilayer perceptron\n",
        "\n",
        "* Look at a different subset of the full dataset (remember the files are orgainsed in blocks of 80)"
      ],
      "metadata": {
        "id": "wW7IakrK2s_n"
      }
    }
  ]
}